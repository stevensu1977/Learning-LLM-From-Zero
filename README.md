# Learning-LLM-From-Zero
This repository is all about my LLM journey: how to use, fine-tuning, and build inference services with LLM.

It's challenging but interesting .

[中文版](#zh_cn/README.md)

# Table of Contents

- [Chapter 1:  Run LLAMA2 model  with Meta official python code](./chapter1.md)
- [Chapter 2: Run LLAMA2 model with HuggingFace transformers](./chapter2.md)
- [Chapter 3:  Embedding your PDF and send it to LLM.](#chapter-3)
- [Chapter 4:  Add vector database Qdrant to your project.](#chapter-4)
- [Chapter 5:  Add LangChain to your project ](#chapter-4)
- [Chapter 6:  Use vLLM build a inference service like openai chatGPT](#chapter-4)
- [Chapter 7:  Use autotrain fine-tuning your model Mijourney/Stable Diffusion prompt](#chapter-4)
- [Reference](#Reference)

## Introduction

Prerequirements:

* Request LLAMA2 access permission and download it , or use other LLAMA2 compatible model, etc. Llama2-Chinese-7b-Chat
* A GPU machine that already install nvidia driver , CUDA ,  I preferred to use AWS EC2 g5.xlarge instance, or other more than 24G GPU memory instance.
* Some python coding and docker skill .



## Chapter 1: Run LLAMA2 model with native python code

This is chapter 1.

## Chapter 2

This is chapter 2.

## Conclusion

This is the conclusion.
