# Learning-LLM-From-Zero
This repository is all about my LLM journey: how to use, fine-tuning, and build inference services with LLM.

It's challenging but interesting .

[中文版](./README.zh_CN.md)

# Table of Contents

- [Chapter 1:  Run LLAMA2 model  with Meta official python code](./en_US/chapter1.md)
- [Chapter 2: Run LLAMA2 model with HuggingFace transformers](./en_US/chapter2.md)
- [Chapter 3:  Embedding your PDF and send it to LLM.](./en_US/chapter3.md)
- [Chapter 4:  Store moer embedding data with vector database Qdrant](./en_US/chapter4.md)
- [Chapter 5  Use vLLM build a inference service like openai chatGPT](./en_US/chapter5.md)
- [Chapter 6:  Fine tuning your own model use autotrain-advanced ](./en_US/chapter6.md)
- [Reference](#Reference)

## Introduction

Prerequirements:

* Request LLAMA2 access permission and download it , or use other LLAMA2 compatible model, etc. Llama2-Chinese-7b-Chat
* A GPU machine that already install nvidia driver , CUDA ,  I preferred to use AWS EC2 g5.xlarge instance, or other more than 24G GPU memory instance.
* Some python coding and docker skill .



## Chapter 1: Run LLAMA2 model with native python code

This is chapter 1.

## Chapter 2

This is chapter 2.

## Conclusion

This is the conclusion.
